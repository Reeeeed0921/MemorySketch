# -*- coding: utf-8 -*-
"""
åˆ†ç±»å…³é”®è¯CSVæ–‡ä»¶æµ‹è¯•
æµ‹è¯•å½’ç±»åçš„å…³é”®è¯æ•°æ®å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import pandas as pd
import json
import requests
from datetime import datetime
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# é…ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_classified_keywords():
    """åŠ è½½åˆ†ç±»åçš„å…³é”®è¯CSVæ–‡ä»¶"""
    try:
        df = pd.read_csv('keywords_classified.csv')
        print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡å…³é”®è¯æ•°æ®")
        return df
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ° keywords_classified.csv æ–‡ä»¶")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

def analyze_classification_distribution(df):
    """åˆ†æåˆ†ç±»åˆ†å¸ƒ"""
    print("\nğŸ“Š åˆ†ç±»åˆ†å¸ƒåˆ†æ:")
    print("=" * 50)
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    category_counts = df['åˆ†ç±»'].value_counts()
    print("ğŸ·ï¸  æŒ‰åˆ†ç±»ç»Ÿè®¡:")
    for category, count in category_counts.items():
        print(f"   {category}: {count} æ¡")
    
    # æŒ‰æƒ…æ„Ÿå€¾å‘ç»Ÿè®¡
    emotion_counts = df['æƒ…æ„Ÿå€¾å‘'].value_counts()
    print("\nğŸ’­ æŒ‰æƒ…æ„Ÿå€¾å‘ç»Ÿè®¡:")
    for emotion, count in emotion_counts.items():
        print(f"   {emotion}: {count} æ¡")
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
    severity_counts = df['ä¸¥é‡ç¨‹åº¦'].value_counts()
    print("\nâš ï¸  æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡:")
    for severity, count in severity_counts.items():
        print(f"   {severity}: {count} æ¡")
    
    # æŒ‰ä½¿ç”¨åœºæ™¯ç»Ÿè®¡
    scene_counts = df['ä½¿ç”¨åœºæ™¯'].value_counts()
    print("\nğŸ¬ æŒ‰ä½¿ç”¨åœºæ™¯ç»Ÿè®¡:")
    for scene, count in scene_counts.head(10).items():
        print(f"   {scene}: {count} æ¡")
    
    return {
        'category_counts': category_counts,
        'emotion_counts': emotion_counts,
        'severity_counts': severity_counts,
        'scene_counts': scene_counts
    }

def generate_keyword_analysis_report(df):
    """ç”Ÿæˆå…³é”®è¯åˆ†ææŠ¥å‘Š"""
    print("\nğŸ“‹ å…³é”®è¯åˆ†ææŠ¥å‘Š:")
    print("=" * 50)
    
    # é«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆä¸¥é‡ç¨‹åº¦ä¸º"é«˜"ï¼‰
    high_priority = df[df['ä¸¥é‡ç¨‹åº¦'] == 'é«˜']
    print(f"ğŸš¨ é«˜ä¼˜å…ˆçº§å…³é”®è¯ ({len(high_priority)} æ¡):")
    for idx, row in high_priority.iterrows():
        print(f"   â€¢ {row['å…³é”®è¯']} ({row['åˆ†ç±»']}) - {row['å»ºè®®å¤„ç†æ–¹å¼']}")
    
    # åŒ»ç–—ç›¸å…³å…³é”®è¯
    medical_keywords = df[df['ä½¿ç”¨åœºæ™¯'] == 'åŒ»ç–—åœºæ™¯']
    print(f"\nğŸ¥ åŒ»ç–—ç›¸å…³å…³é”®è¯ ({len(medical_keywords)} æ¡):")
    for idx, row in medical_keywords.iterrows():
        print(f"   â€¢ {row['å…³é”®è¯']} - {row['å»ºè®®å¤„ç†æ–¹å¼']}")
    
    # æ­£é¢æƒ…æ„Ÿå…³é”®è¯
    positive_keywords = df[df['æƒ…æ„Ÿå€¾å‘'] == 'æ­£é¢']
    print(f"\nğŸ˜Š æ­£é¢æƒ…æ„Ÿå…³é”®è¯ ({len(positive_keywords)} æ¡):")
    for idx, row in positive_keywords.head(10).iterrows():
        print(f"   â€¢ {row['å…³é”®è¯']} ({row['åˆ†ç±»']})")
    
    # æŠ•è¯‰ç›¸å…³å…³é”®è¯
    complaint_keywords = df[df['åˆ†ç±»'].str.contains('æŠ•è¯‰|æŠ±æ€¨', na=False)]
    print(f"\nğŸ“ æŠ•è¯‰ç›¸å…³å…³é”®è¯ ({len(complaint_keywords)} æ¡):")
    for idx, row in complaint_keywords.head(10).iterrows():
        print(f"   â€¢ {row['å…³é”®è¯']} - {row['å»ºè®®å¤„ç†æ–¹å¼']}")

def create_visualization(stats):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # åˆ†ç±»åˆ†å¸ƒé¥¼å›¾
    axes[0, 0].pie(stats['category_counts'].values, labels=stats['category_counts'].index, 
                   autopct='%1.1f%%', startangle=90)
    axes[0, 0].set_title('å…³é”®è¯åˆ†ç±»åˆ†å¸ƒ')
    
    # æƒ…æ„Ÿå€¾å‘æŸ±çŠ¶å›¾
    axes[0, 1].bar(stats['emotion_counts'].index, stats['emotion_counts'].values)
    axes[0, 1].set_title('æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('æƒ…æ„Ÿå€¾å‘')
    axes[0, 1].set_ylabel('æ•°é‡')
    
    # ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
    axes[1, 0].bar(stats['severity_counts'].index, stats['severity_counts'].values)
    axes[1, 0].set_title('ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
    axes[1, 0].set_xlabel('ä¸¥é‡ç¨‹åº¦')
    axes[1, 0].set_ylabel('æ•°é‡')
    
    # ä½¿ç”¨åœºæ™¯åˆ†å¸ƒï¼ˆå‰10ï¼‰
    top_scenes = stats['scene_counts'].head(10)
    axes[1, 1].barh(range(len(top_scenes)), top_scenes.values)
    axes[1, 1].set_yticks(range(len(top_scenes)))
    axes[1, 1].set_yticklabels(top_scenes.index)
    axes[1, 1].set_title('ä½¿ç”¨åœºæ™¯åˆ†å¸ƒï¼ˆå‰10ï¼‰')
    axes[1, 1].set_xlabel('æ•°é‡')
    
    plt.tight_layout()
    plt.savefig('keywords_analysis_report.png', dpi=300, bbox_inches='tight')
    print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸º keywords_analysis_report.png")

def test_with_csv_api(df):
    """ä½¿ç”¨CSVå…³é”®è¯åˆ†æAPIæµ‹è¯•"""
    print("\nğŸ” ä½¿ç”¨CSVå…³é”®è¯åˆ†æAPIæµ‹è¯•:")
    print("=" * 50)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    sample_data = []
    for idx, row in df.head(20).iterrows():
        sample_data.append({
            'text': row['å…³é”®è¯'],
            'user_id': f'user_{idx}',
            'primary_emotion': row['æƒ…æ„Ÿå€¾å‘'],
            'category': row['åˆ†ç±»'],
            'severity': row['ä¸¥é‡ç¨‹åº¦']
        })
    
    try:
        # è°ƒç”¨å…³é”®è¯åˆ†æAPI
        BASE_URL = "http://localhost:5000"
        response = requests.post(f"{BASE_URL}/csv/keywords/analyze", json={
            "data": sample_data,
            "format": "general"
        })
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… APIè°ƒç”¨æˆåŠŸ")
            
            # æ˜¾ç¤ºåˆ†æç»“æœ
            keyword_analysis = result['data']['keyword_analysis']
            print(f"ğŸ“Š åŒ¹é…å…³é”®è¯: {keyword_analysis['matched_records']} / {len(sample_data)}")
            print(f"ğŸ“ˆ å¹³å‡å…³é”®è¯å¯†åº¦: {keyword_analysis['average_keywords_per_record']:.2f}")
            
            # æ˜¾ç¤ºæœ€å¸¸è§å…³é”®è¯
            print("\nğŸ” æœ€å¸¸è§å…³é”®è¯:")
            for keyword, count in list(keyword_analysis['most_common_keywords'].items())[:10]:
                print(f"   â€¢ {keyword}: {count} æ¬¡")
                
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿PGGæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¤±è´¥: {str(e)}")

def export_analysis_results(df, stats):
    """å¯¼å‡ºåˆ†æç»“æœ"""
    print("\nğŸ“¤ å¯¼å‡ºåˆ†æç»“æœ:")
    print("=" * 50)
    
    # åˆ›å»ºåˆ†ææŠ¥å‘Š
    analysis_report = {
        'total_keywords': len(df),
        'categories': stats['category_counts'].to_dict(),
        'emotions': stats['emotion_counts'].to_dict(),
        'severities': stats['severity_counts'].to_dict(),
        'scenes': stats['scene_counts'].to_dict(),
        'high_priority_keywords': df[df['ä¸¥é‡ç¨‹åº¦'] == 'é«˜']['å…³é”®è¯'].tolist(),
        'medical_keywords': df[df['ä½¿ç”¨åœºæ™¯'] == 'åŒ»ç–—åœºæ™¯']['å…³é”®è¯'].tolist(),
        'positive_keywords': df[df['æƒ…æ„Ÿå€¾å‘'] == 'æ­£é¢']['å…³é”®è¯'].tolist(),
        'generated_at': datetime.now().isoformat()
    }
    
    # ä¿å­˜ä¸ºJSON
    with open('keywords_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(analysis_report, f, ensure_ascii=False, indent=2)
    
    print("âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜ä¸º keywords_analysis_report.json")
    
    # å¯¼å‡ºé«˜ä¼˜å…ˆçº§å…³é”®è¯CSV
    high_priority_df = df[df['ä¸¥é‡ç¨‹åº¦'] == 'é«˜']
    high_priority_df.to_csv('high_priority_keywords.csv', index=False, encoding='utf-8')
    print("âœ… é«˜ä¼˜å…ˆçº§å…³é”®è¯å·²å¯¼å‡ºä¸º high_priority_keywords.csv")
    
    # å¯¼å‡ºæŒ‰åˆ†ç±»åˆ†ç»„çš„CSV
    for category in df['åˆ†ç±»'].unique():
        category_df = df[df['åˆ†ç±»'] == category]
        filename = f'keywords_{category}.csv'
        category_df.to_csv(filename, index=False, encoding='utf-8')
        print(f"âœ… {category} å…³é”®è¯å·²å¯¼å‡ºä¸º {filename}")

def create_processing_guidelines(df):
    """åˆ›å»ºå¤„ç†æŒ‡å¯¼æ–¹é’ˆ"""
    print("\nğŸ“‹ å¤„ç†æŒ‡å¯¼æ–¹é’ˆ:")
    print("=" * 50)
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„å¤„ç†å»ºè®®
    severity_guidelines = {
        'é«˜': 'ğŸš¨ ç«‹å³å“åº”ï¼ˆ5åˆ†é’Ÿå†…ï¼‰',
        'ä¸­ç­‰': 'âš ï¸ ä¼˜å…ˆå¤„ç†ï¼ˆ30åˆ†é’Ÿå†…ï¼‰',
        'ä½': 'ğŸ“ æ­£å¸¸å¤„ç†ï¼ˆ2å°æ—¶å†…ï¼‰'
    }
    
    for severity, guideline in severity_guidelines.items():
        count = len(df[df['ä¸¥é‡ç¨‹åº¦'] == severity])
        print(f"{guideline} - {count} æ¡å…³é”®è¯")
    
    # æŒ‰åœºæ™¯åˆ†ç»„å¤„ç†å»ºè®®
    scene_guidelines = {
        'åŒ»ç–—åœºæ™¯': 'ğŸ¥ ç«‹å³è”ç³»åŒ»æŠ¤äººå‘˜',
        'å®¢æˆ·æœåŠ¡': 'ğŸ“ è½¬æ¥å®¢æœä¸»ç®¡',
        'å†²çªåœºæ™¯': 'ğŸš¨ ç´§æ€¥å¤„ç†ï¼Œå®‰æŠšæƒ…ç»ª',
        'æ³•å¾‹åœºæ™¯': 'âš–ï¸ è”ç³»æ³•åŠ¡éƒ¨é—¨',
        'æƒ…ç»ªåœºæ™¯': 'ğŸ’­ å¿ƒç†ç–å¯¼ï¼Œæƒ…æ„Ÿæ”¯æŒ'
    }
    
    print("\nğŸ¬ åœºæ™¯å¤„ç†æŒ‡å¯¼:")
    for scene, guideline in scene_guidelines.items():
        count = len(df[df['ä½¿ç”¨åœºæ™¯'] == scene])
        if count > 0:
            print(f"{guideline} - {count} æ¡å…³é”®è¯")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ åˆ†ç±»å…³é”®è¯CSVæ–‡ä»¶åˆ†æ")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    df = load_classified_keywords()
    if df is None:
        return
    
    # åˆ†æåˆ†ç±»åˆ†å¸ƒ
    stats = analyze_classification_distribution(df)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    generate_keyword_analysis_report(df)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    try:
        create_visualization(stats)
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    # æµ‹è¯•CSV API
    test_with_csv_api(df)
    
    # å¯¼å‡ºåˆ†æç»“æœ
    export_analysis_results(df, stats)
    
    # åˆ›å»ºå¤„ç†æŒ‡å¯¼æ–¹é’ˆ
    create_processing_guidelines(df)
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†ç±»å…³é”®è¯åˆ†æå®Œæˆï¼")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   â€¢ keywords_analysis_report.json - åˆ†ææŠ¥å‘Š")
    print("   â€¢ keywords_analysis_report.png - å¯è§†åŒ–å›¾è¡¨")
    print("   â€¢ high_priority_keywords.csv - é«˜ä¼˜å…ˆçº§å…³é”®è¯")
    print("   â€¢ keywords_*.csv - æŒ‰åˆ†ç±»åˆ†ç»„çš„å…³é”®è¯æ–‡ä»¶")
    print("=" * 60)

if __name__ == "__main__":
    main() 